{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbEZ0-MRIbfF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string, nltk, spacy\n",
    "import os, sys, csv, random, time\n",
    "from unicodedata import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns; sns.set(style='whitegrid')\n",
    "from collections import Counter\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K68E3NU4iJag"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdzum1CGRyaS"
   },
   "source": [
    "# Data PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvQq2QArlfxX"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQt87H4SROsn"
   },
   "outputs": [],
   "source": [
    "def clean_corpus(corpus):\n",
    "\tcleaned = list()\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in corpus:\n",
    "\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\tline = line.decode('UTF-8')\n",
    "\t\tline = line.split()\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\tline = [word.translate(table) for word in line]\n",
    "\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\treturn cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMSf2NOMRObO"
   },
   "outputs": [],
   "source": [
    "def to_vocab(corpus, min_occurance = 0):\n",
    "  tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  vocab = [k for k,v in tokenizer.word_counts.items() if v > min_occurance]\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SY8z3bAfROMh"
   },
   "outputs": [],
   "source": [
    "def update_corpus(corpus, vocab):\n",
    "\tclean_corpus = list()\n",
    "\tfor line in corpus:\n",
    "\t\tnew_tokens = list()\n",
    "\t\tfor token in line.split():\n",
    "\t\t\tif token in vocab:\n",
    "\t\t\t\tnew_tokens.append(token)\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_tokens.append('unk')\n",
    "\t\tnew_line = ' '.join(new_tokens)\n",
    "\t\tclean_corpus.append(new_line)\n",
    "\treturn clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NI2aRsALROJk"
   },
   "outputs": [],
   "source": [
    "def drop_nulls(corpus1, corpus2):\n",
    "    lengths = [len(line) for line in corpus1]\n",
    "    idx = [i for i,line in enumerate(corpus1) if len(line)>0]\n",
    "\n",
    "    corpus1 = [corpus1[i] for i in idx]\n",
    "    corpus2 = [corpus2[i] for i in idx]\n",
    "\n",
    "    return corpus1, corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etobO0PvSDyH"
   },
   "outputs": [],
   "source": [
    "def preprocess(corpus, min_occurance=5):\n",
    "  corpus = clean_corpus(corpus)\n",
    "  vocab = to_vocab(corpus, min_occurance)\n",
    "  corpus = update_corpus(corpus, vocab)\n",
    "  return corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ej2pzwpTo8KO"
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "  corpus = ['<start> '+line+' <end>' for line in corpus]\n",
    "  tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus, target_vocab_size=2**13)\n",
    "  tensor = keras.preprocessing.sequence.pad_sequences([tokenizer.encode(line) for line in corpus],  padding='post')\n",
    "  return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMzpKGoNRjro"
   },
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cIqcEQBRg9y"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/My Drive/Small Vocab/'\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBt-t_uiRijA"
   },
   "outputs": [],
   "source": [
    "en = open(path+files[0],'r').read().split('\\n')\n",
    "fr = open(path+files[1],'r').read().split('\\n')\n",
    "print(len(en), len(fr))\n",
    "print(en[:1])\n",
    "print(fr[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrz2Jn8ZR5AU"
   },
   "outputs": [],
   "source": [
    "en_lengths = [len(line.split()) for line in en]\n",
    "fr_lengths = [len(line.split()) for line in fr]\n",
    "\n",
    "print('Eng:',max(en_lengths), min(en_lengths))\n",
    "print('Fr:',max(fr_lengths), min(fr_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cs25N_oaR49S"
   },
   "outputs": [],
   "source": [
    "en, en_vocab = preprocess(en)\n",
    "fr, fr_vocab = preprocess(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TMyS1kESK6S"
   },
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "  i = random.randint(0, len(en))\n",
    "  print(i,en[i])\n",
    "  print(i,fr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdndHRd0R47l"
   },
   "outputs": [],
   "source": [
    "dump(en, open('/content/drive/My Drive/Datasets/NLP/MT/French-English/Small Vocab/en.pkl', 'wb'))\n",
    "dump(fr, open('/content/drive/My Drive/Datasets/NLP/MT/French-English/Small Vocab/fr.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUC7g7YmlgaN"
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9hdW9KPcOUv"
   },
   "outputs": [],
   "source": [
    "en = load(open('/content/drive/My Drive/Small Vocab/en.pkl', 'rb')) \n",
    "fr = load(open('/content/drive/My Drive/Small Vocab/fr.pkl', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XXBsSB0slMKU",
    "outputId": "3b58181b-a852-4bd8-cc39-1425e386dba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eng: 15 3 \tFr: 21 3\n"
     ]
    }
   ],
   "source": [
    "en_lengths = [len(line.split()) for line in en]\n",
    "fr_lengths = [len(line.split()) for line in fr]\n",
    "print('Eng:',max(en_lengths), min(en_lengths), '\\tFr:',max(fr_lengths), min(fr_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MILD7uqDB-xR"
   },
   "outputs": [],
   "source": [
    "inp_tensor, inp_lang = tokenize(en)\n",
    "targ_tensor, targ_lang = tokenize(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "DByJvXneYNZY",
    "outputId": "581d4e5e-f62a-431c-b809-ff3b565089c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25255 the peach is her favorite fruit but the lemon is their favorite\n",
      "25255 la peche est son fruit prefere mais le citron est leur favori\n",
      "105147 our most loved fruit is the orange but your most loved is the grape\n",
      "105147 nos fruits le plus aime est lorange mais votre plus aime est le raisin\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "  i = random.randint(0, len(en))\n",
    "  print(i,en[i])\n",
    "  print(i,fr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "QSBslRe2CHoN",
    "outputId": "ebf1e729-43ed-415f-8dbc-e2ad3092edb7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max lengths:  21 27\n",
      "Vocab sizes: 543 703\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "buffer_size = 10000\n",
    "batch_size = 64\n",
    "max_input_len = inp_tensor.shape[1]\n",
    "max_targ_len = targ_tensor.shape[1]\n",
    "inp_vocab_size = inp_lang.vocab_size\n",
    "targ_vocab_size = targ_lang.vocab_size\n",
    "print('Max lengths: ', max_input_len, max_targ_len)\n",
    "print('Vocab sizes:', inp_vocab_size, targ_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZtS7AzltiZ9"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(inp_tensor, targ_tensor, test_size=test_size, random_state=42)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size).batch(batch_size)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "Chct-YQTCXKI",
    "outputId": "15af06a2-4a09-4392-ab65-8de2f5738672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21) (64, 27)\n",
      "tf.Tensor(\n",
      "[347   2   4  22   1  12  49   8  84  10   7   1  12  36   6  76   5   3\n",
      " 349   0   0], shape=(21,), dtype=int32) tf.Tensor(\n",
      "[507   2   4  15  33   1  12  53   6  67   9   7   1  12  22   6  88   5\n",
      "   3 509   0   0   0   0   0   0   0], shape=(27,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "inp, targ = next(iter(train_data))\n",
    "print(inp.shape, targ.shape)\n",
    "print(inp[0], targ[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfjL-M5eu1e8"
   },
   "source": [
    "# Transformer Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Hv022khySLS"
   },
   "source": [
    "#### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIY1oPkbfH63"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, embedding_dim):\n",
    "  pos = np.arange(pos)[:, np.newaxis]\n",
    "  angles = 1/np.power(10000, 2*np.arange(embedding_dim)/embedding_dim)[np.newaxis, :]\n",
    "  return pos * angles\n",
    "\n",
    "def positional_encoding(vocab_size, embedding_dim):\n",
    "  angles = get_angles(vocab_size, embedding_dim)\n",
    "  angles[:,::2] = np.sin(angles[:,::2])\n",
    "  angles[:,1::2] = np.cos(angles[:,::2])\n",
    "  pos_encoding = angles[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KjYPf1d7hJ8"
   },
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4iu2chW7ojP"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "  return output, attention_weights\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    assert d_model % self.num_heads == 0\n",
    "    self.depth = d_model // self.num_heads\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "  def split_heads(self, x, batch_size):\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "    return output, attention_weights\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "  def call(self, x, training, mask):\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    return out2\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    return out3, attn_weights_block1, attn_weights_block2\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "  def call(self, x, training, mask):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    x = self.dropout(x, training=training)\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    return x  # (batch_size, input_seq_len, d_model)\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    x = self.dropout(x, training=training)\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,look_ahead_mask, padding_mask)\n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    return x, attention_weights\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRhS_OM6yNQW"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hw0HubAF3Vqj"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "d_model = 256\n",
    "dff = 1024\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "max_len = max([inp_tensor.shape[1], targ_tensor.shape[1]])\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsVbNLRVvM8n"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "train_loss, test_loss = tf.keras.metrics.Mean(name='train_loss'), tf.keras.metrics.Mean(name='test_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFJDOWBN5OSI"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff, inp_vocab_size, targ_vocab_size, pe_input=inp_vocab_size, pe_target=targ_vocab_size, rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1-wKZCi5vhX"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/content/drive/My Drive/Datasets/NLP/Checkpoints/Transformer\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EkjDCL4z6lV"
   },
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "# @tf.function()\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DO4p-oTSMjtU"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  start = time.time()\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  for (batch, (inp, tar)) in enumerate(train_data):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 200 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 10 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9Azs3OcPrdD"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result = evaluate(sentence)\n",
    "  predicted_sentence = targ_lang.decode([i for i in result if i < targ_lang.vocab_size])  \n",
    "\n",
    "  return predicted_sentence\n",
    "  \n",
    "def evaluate(inp_sentence):\n",
    "  inp_sentence = inp_lang.encode(inp_sentence)\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  decoder_input = [507]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(max_len):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "    predictions, _ = transformer(encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    if predicted_id == 509:\n",
    "      return tf.squeeze(output, axis=0)\n",
    "    \n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "  return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "6sm3mNS5OHSD",
    "outputId": "1196ee96-ce1f-4cd5-c549-722c7e5f6812",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> new jersey est parfois calme pendant l automne et il est neigeux en avril <end> <start> new jersey est parfois calme pendant l automne et il est neigeux en avril <end\n"
     ]
    }
   ],
   "source": [
    "inp = inp_lang.decode(inp_tensor[0])\n",
    "tar = targ_lang.decode(targ_tensor[0])\n",
    "pred = translate(inp)\n",
    "print(tar, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JeDmf3sOEql"
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "with open('/content/drive/My Drive/Predictions/predictions.txt','w') as f:\n",
    "  for i, (x,y) in enumerate(zip(x_test, y_test)):\n",
    "    sentence = inp_lang.decode(x)\n",
    "    pred = translate(sentence)\n",
    "    y = ' '.join(targ_lang.decode(y).split(' ')[1:-1])\n",
    "    pred = ' '.join(pred.split(' ')[1:-1])\n",
    "    df += [[y, pred]]\n",
    "    if i % 100 == 0: \n",
    "      print(i)\n",
    "      for line in df:\n",
    "        f.write(line[0]+'\\t'+line[1]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_oywSDP-J_wT"
   },
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('/content/drive/My Drive/Predictions/predictions.txt', sep='\\t', header=None)\n",
    "predictions.drop_duplicates(inplace=True)\n",
    "print(predictions.shape)\n",
    "predictions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8c-Jwxi-uqC"
   },
   "outputs": [],
   "source": [
    "true = predictions[0].values\n",
    "pred = predictions[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "mDO2bperKzJn",
    "outputId": "9cc04b4b-3f48-4b96-cbbe-c885527a40de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7178099241795025\n",
      "0.005937449250641525\n",
      "0.033282942197759194\n"
     ]
    }
   ],
   "source": [
    "score = [0,0,0]\n",
    "for i, (y,p) in enumerate(zip(true,pred)):\n",
    "  score[0] += sentence_bleu(y, p)\n",
    "  score[1] += sentence_bleu(y, p, smoothing_function=SmoothingFunction().method1)\n",
    "  score[2] += sentence_bleu(y, p, smoothing_function=SmoothingFunction().method2)\n",
    "\n",
    "  if i % 1000 == 0: print(i)\n",
    "print(score[0]/predictions.shape[0])\n",
    "print(score[1]/predictions.shape[0])\n",
    "print(score[2]/predictions.shape[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qvQq2QArlfxX",
    "KUC7g7YmlgaN",
    "0DPbyPBEikKf",
    "K0t-WZ_8k2wN",
    "kUlD_DDIojIW",
    "2fN_Yn9Qoouk",
    "3indC7nTosVC",
    "VjhKJBSsoyXq",
    "HFsrKTC436AY",
    "i7WlkNGCo0Pb",
    "YNimCTnyo2Mr",
    "4Hv022khySLS",
    "5KjYPf1d7hJ8"
   ],
   "name": "MT_Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
